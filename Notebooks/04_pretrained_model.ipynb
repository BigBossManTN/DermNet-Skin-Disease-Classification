{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4087061-e6eb-4573-9ca6-3d1f113358bf",
   "metadata": {},
   "source": [
    "1. Pretrained Model Choice\n",
    "\n",
    "- Select a pretrained CNN model such as VGG16, MobileNetV2 or ResNet50.\n",
    "- Load the model with ImageNet weights.\n",
    "- Freeze the base layers to use it as a feature extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266d64b-1111-4287-9c03-430fdeb3a3fa",
   "metadata": {},
   "source": [
    "*Pretrained Model Choice*\n",
    "\n",
    "MobileNetV2 is used as the pretrained base model because it is lightweight,\n",
    "fast to train and commonly used for medical image classification. \n",
    "The base layers will be frozen first to use the network as a feature extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c277070-dd41-4128-8f75-4fd304caf871",
   "metadata": {},
   "source": [
    "2. Data Generators\n",
    "- Use the same data generators as the baseline model.\n",
    "- Make sure the image size matches the pretrained model requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e759b89e-f148-42bf-8ef1-3522084e7f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12453 images belonging to 23 classes.\n",
      "Found 3104 images belonging to 23 classes.\n",
      "Found 4002 images belonging to 23 classes.\n",
      "Number of classes: 23\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Imports and Load Generators\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "train_dir = 'E:/CY Tech/Big Data project/Project 2 DermNet Skin Disease Classification/Data/Raw/train'\n",
    "test_dir = 'E:/CY Tech/Big Data project/Project 2 DermNet Skin Disease Classification/Data/Raw/test'\n",
    "\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 32\n",
    "\n",
    "# Use augmentation for training (pretrained models need it)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Training subset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Validation subset\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Test generator\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d05ec-2629-4f3c-948a-c3361f5a0402",
   "metadata": {},
   "source": [
    "3. Model Architecture\n",
    "\n",
    "- Add custom layers on top of the pretrained base:\n",
    "  - GlobalAveragePooling2D\n",
    "  - Dense layers\n",
    "  - Final softmax layer matching number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b26d91ce-5474-4921-a6ce-c53bf2d88269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,967</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)             │         \u001b[38;5;34m2,967\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,424,919</span> (9.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,424,919\u001b[0m (9.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">166,935</span> (652.09 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m166,935\u001b[0m (652.09 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2 :Build the Pretrained Model Architecture\n",
    "\n",
    "# Load the pretrained MobileNetV2 base\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=(img_height, img_width, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze base layers (important for transfer learning)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build the classifier on top\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a180c-5629-487b-a04f-18ee00f67cc6",
   "metadata": {},
   "source": [
    "4. Training\n",
    "\n",
    "- Train only the top layers first.\n",
    "- Optionally unfreeze some deeper layers for fine tuning.\n",
    "- Track accuracy and loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c73d3809-23b6-4e0b-8713-0a54de4c1c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 604ms/step - accuracy: 0.2533 - loss: 2.5377 - val_accuracy: 0.2220 - val_loss: 2.6825\n",
      "Epoch 2/5\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 594ms/step - accuracy: 0.3239 - loss: 2.2635 - val_accuracy: 0.2310 - val_loss: 2.6580\n",
      "Epoch 3/5\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 591ms/step - accuracy: 0.3573 - loss: 2.1298 - val_accuracy: 0.2465 - val_loss: 2.6208\n",
      "Epoch 4/5\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 594ms/step - accuracy: 0.3938 - loss: 2.0326 - val_accuracy: 0.2497 - val_loss: 2.6340\n",
      "Epoch 5/5\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 589ms/step - accuracy: 0.4035 - loss: 1.9682 - val_accuracy: 0.2490 - val_loss: 2.6503\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Training (Top Layers Only)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0005),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "epochs = 5  # safe and not too long\n",
    "\n",
    "history_pretrained = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba94c2-62d7-4aa1-8bd4-61f3a88e684f",
   "metadata": {},
   "source": [
    "5. Evaluation\n",
    "\n",
    "- Evaluate the pretrained model on the test set.\n",
    "- Compare performance with the baseline CNN.\n",
    "- Generate accuracy, confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e69867-711a-44d4-8ff9-7dd333d50e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 234ms/step - accuracy: 0.3476 - loss: 2.2470\n",
      "Test accuracy (pretrained model): 0.34757620096206665\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 241ms/step\n",
      "Confusion Matrix:\n",
      "[[200   8   3   0   0   5   2   5   7   1   0   3   0   2  11   0  36   1\n",
      "   18   1   0   0   9]\n",
      " [ 26  68   5   1   1   9   1   1   7   0   1   5   1   0  15   0 106   9\n",
      "   12   1   1   4  14]\n",
      " [ 14   1  28   0   0  18   7   2   1   2   1   0   0   0   7   0   7   8\n",
      "    9   3   3   8   4]\n",
      " [  8  14   3   3   0   6   0   2   6   0   2   1   1   0  18   0  14   8\n",
      "    9   1   1   6  10]\n",
      " [ 11   2   3   0   2   0   5   2   3   0   2   0   1   0   2   0  11   6\n",
      "   12   1   0   4   6]\n",
      " [ 14   6   4   2   0 129   3   3   5   2   0   2   7   0  23   1  30   8\n",
      "   42   0   1  17  10]\n",
      " [  3   0   6   2   0   7  32   1   2   1   1   0   0   0  15   0   3   6\n",
      "   10   2   0   6   4]\n",
      " [  3   0   1   0   0   0   1  32   1   0   0   0   0   0   4   0  11   1\n",
      "    6   0   0   0   0]\n",
      " [  1   3   2   2   0   5   0   7  29   0   1   0   2   0   5   1  14   6\n",
      "    6   0   2   2  14]\n",
      " [ 16   4   9   0   1  18   5   4   4  11   1   1   0   0  16   0  15  15\n",
      "   14   0   0   6   3]\n",
      " [ 10   9   4   1   0  10   7   4   0   5   6   0   3   0   6   1  13   8\n",
      "   10   0   1   2   5]\n",
      " [  2   1   4   0   0   1   1   4   2   0   0  40   0   0   2   1  43   4\n",
      "    3   0   2   1   5]\n",
      " [  3   2   2   1   0   6   0  16   0   0   0   0 194   0   8   1   8   1\n",
      "    6   0   3   1   9]\n",
      " [  8   0   5   0   0  11   3   1   4   0   1   2   0   2   7   0   8   1\n",
      "    4   0   1   5   2]\n",
      " [ 19   8   3   4   0  43   3  13  13   1   0   4   3   0  92   3  50  10\n",
      "   43   0   0  13  27]\n",
      " [  3   3   7   2   0   8   2   3   4   0   0   3   0   0   9  13   9   9\n",
      "   14   1   1  11   6]\n",
      " [ 17  15   3   1   0  13   5   6   6   0   0  25   0   0  14   6 192   9\n",
      "   13   0   1   1  16]\n",
      " [ 10   4   0   0   0   8   7   1   4   0   3   0   5   0   7   1  28  35\n",
      "   14   0   0  19   6]\n",
      " [ 32   7   5   1   0  27   8  11  12   2   1   1   1   0  26   1  28  10\n",
      "  131   0   0   5  16]\n",
      " [  6   0   3   0   0   2   2   0   1   0   0   0   0   0  15   0   1   5\n",
      "   11   2   0   4   1]\n",
      " [ 17  10   2   1   0   3   1   1   3   0   1   2   0   0   4   1  34   4\n",
      "    9   0  17   2   9]\n",
      " [  1   0   2   0   0   7   5   0   1   0   0   0   0   0   7   0   6   6\n",
      "   10   0   1  44  15]\n",
      " [ 20   5   5   0   0  11   4   2   9   0   1   6   9   2  17   1  48  11\n",
      "   20   0   2  10  89]]\n",
      "Classification Report:\n",
      "                                                                    precision    recall  f1-score   support\n",
      "\n",
      "                                           Acne and Rosacea Photos       0.45      0.64      0.53       312\n",
      "Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions       0.40      0.24      0.30       288\n",
      "                                          Atopic Dermatitis Photos       0.26      0.23      0.24       123\n",
      "                                            Bullous Disease Photos       0.14      0.03      0.04       113\n",
      "                Cellulitis Impetigo and other Bacterial Infections       0.50      0.03      0.05        73\n",
      "                                                     Eczema Photos       0.37      0.42      0.39       309\n",
      "                                      Exanthems and Drug Eruptions       0.31      0.32      0.31       101\n",
      "                 Hair Loss Photos Alopecia and other Hair Diseases       0.26      0.53      0.35        60\n",
      "                                  Herpes HPV and other STDs Photos       0.23      0.28      0.26       102\n",
      "                      Light Diseases and Disorders of Pigmentation       0.44      0.08      0.13       143\n",
      "                        Lupus and other Connective Tissue diseases       0.27      0.06      0.09       105\n",
      "                               Melanoma Skin Cancer Nevi and Moles       0.42      0.34      0.38       116\n",
      "                                Nail Fungus and other Nail Disease       0.85      0.74      0.80       261\n",
      "                    Poison Ivy Photos and other Contact Dermatitis       0.33      0.03      0.06        65\n",
      "             Psoriasis pictures Lichen Planus and related diseases       0.28      0.26      0.27       352\n",
      "             Scabies Lyme Disease and other Infestations and Bites       0.42      0.12      0.19       108\n",
      "                      Seborrheic Keratoses and other Benign Tumors       0.27      0.56      0.36       343\n",
      "                                                  Systemic Disease       0.19      0.23      0.21       152\n",
      "            Tinea Ringworm Candidiasis and other Fungal Infections       0.31      0.40      0.35       325\n",
      "                                                   Urticaria Hives       0.17      0.04      0.06        53\n",
      "                                                   Vascular Tumors       0.46      0.14      0.22       121\n",
      "                                                 Vasculitis Photos       0.26      0.42      0.32       105\n",
      "                        Warts Molluscum and other Viral Infections       0.32      0.33      0.32       272\n",
      "\n",
      "                                                          accuracy                           0.35      4002\n",
      "                                                         macro avg       0.34      0.28      0.27      4002\n",
      "                                                      weighted avg       0.36      0.35      0.33      4002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5 :evaluate Pretrained Model on Test Set\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# evaluate test accuracy\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(\"Test accuracy (pretrained model):\", test_acc)\n",
    "\n",
    "# get predictions\n",
    "pred_probs = model.predict(test_generator)\n",
    "pred_classes = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "# true labels\n",
    "true_classes = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(true_classes, pred_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_classes, pred_classes, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb0cf42-2219-436f-863a-f4dddbe85488",
   "metadata": {},
   "source": [
    "6. Comparison With Baseline\n",
    "\n",
    "- Summarize how much improvement the pretrained model brings.\n",
    "- Explain why it performs better or worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e3725-8ff8-41b6-805d-9f46d75b3f90",
   "metadata": {},
   "source": [
    "- The pretrained MobileNetV2 model performed better than the baseline CNN. \n",
    "- The baseline CNN was simple and had limited capacity, so it struggled to pick up \n",
    "the complex texture patterns that appear in medical skin images. \n",
    "- MobileNetV2 already learned rich features from ImageNet, so even with only the top \n",
    "layers trained, it was able to generalize better.\n",
    "\n",
    "- The pretrained model improved the overall accuracy and gave stronger class-level \n",
    "precision and recall. It also handled the class imbalance better because its \n",
    "feature extractor is stronger. The baseline model underfit, while the pretrained \n",
    "one showed more stable validation performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb4e287-a3b8-437b-a51d-068414088f63",
   "metadata": {},
   "source": [
    "7. Final Model Selection\n",
    "\n",
    "- Choose the best performing model.\n",
    "- Save it for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8172a0c-8ccb-45fc-b190-ccb2b80a180a",
   "metadata": {},
   "source": [
    "- The pretrained MobileNetV2 model was selected as the final model because it gave\n",
    "better performance than the baseline CNN. It reached higher accuracy on the test set\n",
    "and produced stronger class level scores in the classification report. The model \n",
    "generalized better thanks to the pretrained feature extractor and the data augmentation.\n",
    "\n",
    "- This model is saved for later use so it can be reused in the evaluation notebook or \n",
    "in a small application if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9878c82-f93b-45dc-b9c6-edbf0e9162bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "#save final model\n",
    "model.save('E:/CY Tech/Big Data project/Project 2 DermNet Skin Disease Classification/Models/final_mobilenet_model.h5')\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "567b4aef-f55a-4675-a955-af2a711f4e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully in Keras format.\n"
     ]
    }
   ],
   "source": [
    "model.save('E:/CY Tech/Big Data project/Project 2 DermNet Skin Disease Classification/Models/final_mobilenet_model.keras')\n",
    "print(\"Model saved successfully in Keras format.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
